# Filename: W1308_GenAI_LC_RAG_08_streamlit.py
# ========================
# LangChain RAG + Streamlit GUI ä»‹é¢
# ========================

#!pip install pypdf chromadb streamlit

import os
import datetime
from dotenv import load_dotenv
import openai
import streamlit as st

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import (RecursiveCharacterTextSplitter)
from langchain_chroma import Chroma
from langchain.schema import HumanMessage

def init_environment():
    load_dotenv(override=True)
    os.environ["USER_AGENT"] = "LangChain-RAG-Agent/1.0"
    openai.api_key = os.getenv("OPENAI_API_KEY")
    return openai.api_key, os.getenv("GEMINI_API_KEY")

def init_llms(openai_key):
    openai_llm = ChatOpenAI(model_name="gpt-3.5-turbo", openai_api_key=openai_key)
    gemini_llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", temperature=0.7)
    return openai_llm, gemini_llm

def ensure_directory(path):
    if not os.path.exists(path):
        os.makedirs(path)

# ä½¿ç”¨ LangChain çš„ PyPDFLoader å°‡ PDF è§£ææˆæ–‡æœ¬ã€‚
# ä½¿ç”¨ RecursiveCharacterTextSplitter æ‹†åˆ†æˆå¯åµŒå…¥ï¼ˆembeddingï¼‰çš„ chunksã€‚
def load_and_split_pdf(pdf_path, chunk_size=500, chunk_overlap=20):
    loader = PyPDFLoader(file_path=pdf_path)
    docs = loader.load()
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    return docs, splitter.split_documents(docs)

#æŠŠ chunks è½‰æˆå‘é‡ä¸¦å­˜å…¥æœ¬åœ° ./chroma_langchain_db ä¸­ï¼Œå¯è¢«ç”¨æ–¼æª¢ç´¢å¼å•ç­”ï¼ˆRAGï¼‰ã€‚
def create_chroma_db(chunks, embedding_model, db_path):
    return Chroma.from_documents(
        documents=chunks,
        embedding=embedding_model,
        persist_directory=db_path,
        collection_metadata={"hnsw:space": "cosine"}
    )

# ä½¿ç”¨ Retriever æ‰¾å‡ºèˆ‡å•é¡Œæœ€ç›¸é—œçš„å…§å®¹ã€‚
# ç”¨ prompt æ¨¡æ¿å°‡ä¸Šä¸‹æ–‡èˆ‡å•é¡ŒåŒ…è£çµ¦ LLM å›ç­”ã€‚
def retrieve_and_ask(llm, retriever, query):
    docs = retriever.invoke(query)
    context = "\n".join([doc.page_content for doc in docs if hasattr(doc, "page_content")])
    prompt = f"è«‹æ ¹æ“šä»¥ä¸‹å…§å®¹å›ç­”å•é¡Œ:\n\n{context}\n\nå•é¡Œ: {query}"
    return llm.invoke([HumanMessage(content=prompt)]), context

def save_output(question, context, answer, output_path="./output"):
    ensure_directory(output_path)
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = os.path.join(output_path, f"rag_result_{timestamp}.txt")
    with open(filename, "w", encoding="utf-8") as f:
        f.write("[Query]\n" + question + "\n\n")
        f.write("[Retrieved Context]\n" + context + "\n\n")
        f.write("[Answer]\n" + answer + "\n")
    return filename

def print_assignment_info(student_id):
    print("\n-----------------------------")
    print(f"W13 Assignment 8 | Student ID: {student_id} | {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("-----------------------------\n")


def main():
    student_id = "YourStudentID1234"  # â† è«‹åœ¨é€™è£¡å¡«ä¸Šä½ çš„å­¸è™Ÿ

    st.set_page_config(page_title="ğŸš¦ Car Law RAG QA System", layout="wide")
    st.title("ğŸš¦ è»Šè¼›æ³•è¦ RAG å•ç­”ç³»çµ±")
    st.markdown("è¼¸å…¥èˆ‡è»Šè¼›æ³•è¦ç›¸é—œçš„å•é¡Œï¼Œç³»çµ±å°‡å¾ PDF æª”æ¡ˆä¸­æ‰¾å‡ºè³‡æ–™ä¸¦å›ç­”ã€‚")

    openai_key, _ = init_environment()
    openai_llm, _ = init_llms(openai_key)

    db_path = "./chroma_langchain_db"
    ensure_directory(db_path)

    uploaded_file = st.file_uploader("ğŸ“„ ä¸Šå‚³æ‚¨è¦æŸ¥è©¢çš„ PDF æ³•è¦æ–‡ä»¶ï¼š", type="pdf")

    if uploaded_file:
        with st.spinner("è¼‰å…¥æ³•è¦è³‡æ–™ä¸­..."):
            with open("./temp_uploaded.pdf", "wb") as f:
                f.write(uploaded_file.getbuffer())
            docs, chunks = load_and_split_pdf('./temp_uploaded.pdf')
            embeddings = OpenAIEmbeddings(model='text-embedding-3-large')
            db = create_chroma_db(chunks, embeddings, db_path)
            retriever = db.as_retriever(search_type="similarity", search_kwargs={"k": 6})

        query = st.text_area("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œï¼ˆç¹é«”ä¸­æ–‡ï¼‰:", "é…’å¾Œé–‹è»Šä¸”é…’ç²¾æ¿ƒåº¦è¶…éè¦å®šæ¨™æº–æ‡‰ç½°æ¬¾å¤šå°‘?", height=100)

        if st.button("ğŸš€ æŸ¥è©¢"):
            with st.spinner("æ­£åœ¨æŸ¥è©¢ä¸­..."):
                response, context = retrieve_and_ask(openai_llm, retriever, query)
                st.success("âœ… å›ç­”å®Œæˆï¼")
                st.subheader("ğŸ”¹ å›ç­”å…§å®¹")
                st.write(response.content)

                with st.expander("ğŸ” æª¢ç´¢å…§å®¹ï¼ˆContextï¼‰"):
                    st.code(context)

                filename = save_output(query, context, response.content)
                st.download_button("ğŸ’¾ ä¸‹è¼‰çµæœæª”æ¡ˆ", data=open(filename, "r", encoding="utf-8").read(), file_name=os.path.basename(filename))
    else:
        st.warning("è«‹å…ˆä¸Šå‚³ä¸€ä»½ PDF æ–‡ä»¶ä»¥å•Ÿå‹•æŸ¥è©¢åŠŸèƒ½ã€‚")

    print_assignment_info(student_id)

if __name__ == "__main__":
    main()
